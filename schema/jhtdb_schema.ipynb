{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3da6aaad-5d8c-462d-b5f1-440d89996691",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T18:38:28.136355Z",
     "iopub.status.busy": "2025-02-19T18:38:28.136107Z",
     "iopub.status.idle": "2025-02-19T18:38:28.352394Z",
     "shell.execute_reply": "2025-02-19T18:38:28.351629Z",
     "shell.execute_reply.started": "2025-02-19T18:38:28.136302Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from enum import Enum\n",
    "from typing import Optional, List, Dict, Union, Literal\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\"\"\"\n",
    "The JHTDB Data Model: pydantic classes.\n",
    "\"\"\"\n",
    "class Offset(BaseModel):\n",
    "    name: str\n",
    "    grid: List[float]\n",
    "    coordinate: List[float]\n",
    "\n",
    "class Feature(BaseModel):\n",
    "    \"\"\"\n",
    "    general quadruple basically, used for listing the various featues (methods, operators etc) that are\n",
    "    available on a Dataset.\n",
    "    the code is to be used as identifier and references to a Feature in a certain collection.\n",
    "    \"\"\"\n",
    "    code: str\n",
    "    name: str\n",
    "    id: Optional[str] = None\n",
    "    description: Optional[str] = None\n",
    "\n",
    "class Variable(Feature):\n",
    "    component_codes: List[str]\n",
    "    cardinality: int\n",
    "\n",
    "class VariableOperatorMethod(BaseModel):\n",
    "    \"\"\"\n",
    "    which spatial interpolation methods can be applied to the result of which operator applied to which variable.\n",
    "    all represented by their codes pointing to the variables/operators/interpolation_methods in the database definition.\n",
    "    \"\"\"\n",
    "    operator: str\n",
    "    methods: List[str]\n",
    "\n",
    "class PhysicalVariable(BaseModel):\n",
    "    code: str\n",
    "    name: str\n",
    "    # whether the variable is stored for each grid point on disk.\n",
    "    gridded: bool\n",
    "    # staggered grid and coordinate offsets between dimensions.\n",
    "    offsets: List[Offset]\n",
    "    spatialOperatorMethods: List[VariableOperatorMethod]\n",
    "    temporalMethods: List[str]\n",
    "\n",
    "class TimeIndexShift(BaseModel):\n",
    "    getcutout: int\n",
    "    getdata: int\n",
    "\n",
    "class Dimension(BaseModel):\n",
    "    # physical lower bound on dimension.\n",
    "    lower: str\n",
    "    # physical upper bound on dimension. str because it may have \"pi\" included.\n",
    "    upper: str\n",
    "    # number of cells along the dimension.\n",
    "    n: int\n",
    "    # whether the dimension is stored as discrete steps.\n",
    "    discrete: Optional[bool] = None\n",
    "    # whether the dimension boundary is periodic.\n",
    "    isPeriodic: Optional[bool] = False\n",
    "    # shift applied to the user-specified time index.\n",
    "    # differs based on query type and whether or not pchip time interpolation is allowed.\n",
    "    timeIndexShift: Optional[TimeIndexShift] = None\n",
    "    # grid spacing [dx, dy, dz].\n",
    "    # irregular grid spacing dimensions are specified as a string the grid values are stored in python.\n",
    "    spacing: Optional[Union[str, float]] = None\n",
    "    # zarr chunk size.\n",
    "    chunk: Optional[int] = None\n",
    "\n",
    "class Simulation(BaseModel):\n",
    "    tlims: Dimension\n",
    "    xlims: Dimension\n",
    "    ylims: Dimension\n",
    "    zlims: Dimension\n",
    "\n",
    "class Dataset(BaseModel):\n",
    "    displayname: str\n",
    "    name: str\n",
    "    simulation: Simulation\n",
    "    description: Optional[str] = None\n",
    "    # list of codes of variables available in this dataset.\n",
    "    physicalVariables: List[PhysicalVariable]\n",
    "\n",
    "class TurbulenceDB(BaseModel):\n",
    "    name: str\n",
    "    description: Optional[str] = None\n",
    "    variables: List[Variable]\n",
    "    # operators are derived fields that can be extracted in addition to the original field.\n",
    "    # examples are hessian, gradient, laplacian.\n",
    "    spatial_operators: List[Feature]\n",
    "    # different spatial interpolation methods can be applied for point queries.\n",
    "    # examples are lag4, lag6, lag8, m2q8, fd4lag4, fd6noint.\n",
    "    spatial_methods: List[Feature]\n",
    "    # time interpolation methods.\n",
    "    # examples are none, pchip.\n",
    "    temporal_methods: List[Feature]\n",
    "    datasets: List[Dataset]\n",
    "    \n",
    "\"\"\"\n",
    "GL these classes can be used server side.\n",
    "identify a config file describing the datasets and adds some information relevant for server side actions.\n",
    "\"\"\"\n",
    "class StorageDescriptor(BaseModel):\n",
    "    storageType : Literal['TBD'] = 'TBD'\n",
    "    \n",
    "class LegacDBStorage(StorageDescriptor):\n",
    "    storageType : Literal['LegacyDB'] = 'LegacyDB'\n",
    "    turbinfoDatabaseURL: str\n",
    "    \n",
    "class CephZARRStorage(StorageDescriptor):\n",
    "    storageType : Literal['ZARR'] = 'ZARR'\n",
    "    cephParentDirectoryPath: str\n",
    "\n",
    "class FileDBStorage(StorageDescriptor):\n",
    "    storageType : Literal['FileDB'] = 'FileDB'\n",
    "    filedbPickledMDFilePath: str\n",
    "\n",
    "class DatasetStorageDescriptor(BaseModel):\n",
    "    datasetName: str\n",
    "    storageDescriptor: Union[LegacDBStorage, CephZARRStorage, FileDBStorage, StorageDescriptor] = Field(discriminator='storageType')\n",
    "\n",
    "class JHTDBServerSide(BaseModel):\n",
    "    jhtdbConfigFileURL: str\n",
    "    datasets: List[DatasetStorageDescriptor]\n",
    "\n",
    "\"\"\"\n",
    "client side config.\n",
    "config files that add application specific metadata to the datasets.    \n",
    "\"\"\"\n",
    "class CoordinateEnum(str, Enum):\n",
    "    T = 't'\n",
    "    X = 'x'\n",
    "    Y = 'y'\n",
    "    Z = 'z'\n",
    "\n",
    "\"\"\"\n",
    "cutout service config.\n",
    "\"\"\"\n",
    "class CutoutLimit(BaseModel):\n",
    "    coordinate: CoordinateEnum\n",
    "    lower: float\n",
    "    upper: float\n",
    "    default_lower: float\n",
    "    default_upper: float\n",
    "    \n",
    "class DatasetCutout(BaseModel):\n",
    "    datasetName: str\n",
    "    cutout_variables: List[str]\n",
    "    coordinate_lims: List[CutoutLimit]\n",
    "\n",
    "class JHTDBCutout(BaseModel):\n",
    "    jhtdbConfigFileURL: str\n",
    "    datasets: List[DatasetCutout]\n",
    "        \n",
    "\"\"\"      \n",
    "point queries config.\n",
    "\"\"\"   \n",
    "class CoordinateValue(BaseModel):\n",
    "    coordinate: CoordinateEnum\n",
    "    value: str  # can conain 'pi'\n",
    "\n",
    "class DatasetDefaults(BaseModel):\n",
    "    datasetName: str\n",
    "    default_coordinates: List[CoordinateValue]\n",
    "    \n",
    "class JHTDBPointQuery(BaseModel):\n",
    "    jhtdbConfigFileURL: str\n",
    "    datasets: List[DatasetDefaults]\n",
    "    \n",
    "\"\"\"\n",
    "generate TurbulenceDB schema file.\n",
    "\"\"\"\n",
    "schema = TurbulenceDB.model_json_schema()\n",
    "\n",
    "# save to file.\n",
    "with open('/home/idies/workspace/Storage/mschnau1/persistent/giverny/pydantic_json/jhtdb-schema.json', 'w') as f:\n",
    "    json.dump(schema, f, indent = 2)\n",
    "    \n",
    "\"\"\"\n",
    "generate JHTDB cutout schema file.\n",
    "\"\"\"\n",
    "schema = JHTDBCutout.model_json_schema()\n",
    "\n",
    "# save to file.\n",
    "with open('/home/idies/workspace/Storage/mschnau1/persistent/giverny/pydantic_json/jhtdb-cutout-schema.json', 'w') as f:\n",
    "    json.dump(schema, f, indent = 2)\n",
    "    \n",
    "\"\"\"\n",
    "generate JHTDB points schema file.\n",
    "\"\"\"\n",
    "schema = JHTDBPointQuery.model_json_schema()\n",
    "\n",
    "# save to file.\n",
    "with open('/home/idies/workspace/Storage/mschnau1/persistent/giverny/pydantic_json/jhtdb-points-schema.json', 'w') as f:\n",
    "    json.dump(schema, f, indent = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d4dc38d-f2d9-4ef4-a95d-8fd6338b1a12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T18:38:32.563654Z",
     "iopub.status.busy": "2025-02-19T18:38:32.563287Z",
     "iopub.status.idle": "2025-02-19T18:38:32.570676Z",
     "shell.execute_reply": "2025-02-19T18:38:32.570118Z",
     "shell.execute_reply.started": "2025-02-19T18:38:32.563636Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config is valid!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# load your config file.\n",
    "with open('/home/idies/workspace/Storage/mschnau1/persistent/giverny/pydantic_json/jhtdb-config.json', 'r') as f:\n",
    "    config_data = json.load(f)\n",
    "\n",
    "# validate it.\n",
    "try:\n",
    "    db = TurbulenceDB(**config_data)\n",
    "    print(\"config is valid!\")\n",
    "except Exception as e:\n",
    "    print(f\"validation error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c924d16-76c4-40ae-8cdb-e6cee49a8658",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (py39)",
   "language": "python",
   "name": "py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
